{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Darshan2104/CE_135_Darshan_Tank_ML/blob/main/Lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c_wNWGbQwMg",
    "outputId": "388e326f-1cfb-4ebf-d6f8-2265d0ddacb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7OyrDAlkQxI4",
    "outputId": "b8d90885-801d-4f42-bd78-af70b3926c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import twitter_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "v-mItkT_RFJt"
   },
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "tw_data = all_positive_tweets + all_negative_tweets\n",
    "tw_label = np.append(np.ones((len(all_positive_tweets),1)), np.zeros((len(all_negative_tweets),1)),axis=0)\n",
    "\n",
    "train_x,test_x,train_y,test_y = train_test_split(tw_data,tw_label,test_size=0.30,random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YrYEwkLaRLjT"
   },
   "outputs": [],
   "source": [
    "class MyTweetLogisticRegressionModel:\n",
    "  def __transform_data(self,tweets):\n",
    "    data = []\n",
    "    for tweet in tweets:\n",
    "      data.append(self.extract_features(tweet))\n",
    "    return np.asarray(data)\n",
    "\n",
    "  def getTransformedData(self,data):\n",
    "    return self.__transform_data(data)\n",
    "\n",
    "\n",
    "\n",
    "  def fit(self, x, y, alpha=1e-4, epochs=10,thresval=0.5,record_epoch=False):\n",
    "    self.thresval = thresval\n",
    "    self._freq = self.get_freqs(x,y)\n",
    "    self._shape = 2\n",
    "    self._inputs = self.__transform_data(x)\n",
    "    self._tlen = len(y)\n",
    "    self._bias = tf.Variable(np.random.randn(1), name = \"Bias\")\n",
    "    self._weights = tf.Variable(np.random.randn(1,self._shape), name =\"Weight\")\n",
    "    self._tinit = tf.compat.v1.global_variables_initializer()\n",
    "    self._targets = np.asarray(y)\n",
    "    self.alpha = alpha\n",
    "    self.epochs = epochs\n",
    "    self._repoch = record_epoch\n",
    "    self.__trainModel()\n",
    "    return\n",
    "  def __initSaver(self):\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "    self._saver = saver\n",
    "    self._sesspath = 'TSession'\n",
    "    return\n",
    "  def threshold(self,val):\n",
    "    if val>=self.thresval:\n",
    "      return 1\n",
    "    return 0\n",
    "\n",
    "  def process_tweet(self,tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "    tweets_clean = []\n",
    "\n",
    "\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "      if word not in stopwords_english and word not in string.punctuation:\n",
    "        tweets_clean.append(stemmer.stem(word))\n",
    "    return tweets_clean\n",
    "  def predict(self, indata):\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "      self._saver.restore(sess,save_path=self._sesspath)\n",
    "      indata = self.__transform_data(indata)\n",
    "      return sess.run(tf.nn.sigmoid(tf.add(tf.matmul(a=indata,b=self._weights,transpose_b=True),self._bias)))\n",
    "    print(\"Failed To Retrieve Session\")\n",
    "    return\n",
    "\n",
    "  def get_accuracy(self,x,y):\n",
    "    if len(x)!=len(y):\n",
    "      print(\"Dimension are different.\")\n",
    "      return\n",
    "    return ((x==y).sum())/len(y)\n",
    "\n",
    "  def get_freqs(self,tweets, ys):\n",
    "    freqs = dict()\n",
    "    yslist=np.squeeze(ys).tolist()\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "      for word in self.process_tweet(tweet):\n",
    "        pair = (word, y)\n",
    "        if pair not in freqs:\n",
    "          freqs[pair]=0\n",
    "        freqs[pair]+=1\n",
    "    return freqs\n",
    "\n",
    "  def extract_features(self,tweet):\n",
    "    freqs = self._freq\n",
    "    word_l = self.process_tweet(tweet)\n",
    "    x = np.zeros((1, 2))\n",
    "    for word in word_l:\n",
    "      if (word,1) in freqs:\n",
    "        x[0,0]+= freqs[word,1]\n",
    "      if (word,0) in freqs:\n",
    "        x[0,1] += freqs[word,0]\n",
    "    assert(x.shape == (1, 2))\n",
    "    return x[0]\n",
    "  def getAccDets(self):\n",
    "    if self._repoch:\n",
    "      return self.__accurary_det\n",
    "    return\n",
    "\n",
    "\n",
    "  def __trainModel(self):\n",
    "    if self._repoch:\n",
    "      self._err = []\n",
    "      self._precs = []\n",
    "      self.__accurary_det = []\n",
    "    temp0 = tf.nn.sigmoid(tf.add(tf.matmul(a=self._inputs,b=self._weights,transpose_b=True),self._bias))\n",
    "    err = tf.nn.sigmoid_cross_entropy_with_logits(logits=temp0,labels=self._targets)\n",
    "    temp = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=self.alpha,name='GradientDescent').minimize(err)\n",
    "    self.__initSaver()\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "      sess.run(self._tinit)\n",
    "      print(\"Bias\",sess.run(self._bias))\n",
    "      print(\"Weights\",sess.run(self._weights))\n",
    "      for epoch in range(self.epochs):\n",
    "        sess.run(temp)\n",
    "        __preds = sess.run(temp0)\n",
    "        acc = ((__preds==self._targets).sum())/self._tlen\n",
    "        if self._repoch:\n",
    "          self.__accurary_det.append(acc)\n",
    "        if epoch % 1000 == 0:\n",
    "          print(\"Acc:\",acc)\n",
    "      self._saver.save(sess,self._sesspath)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oGq3nLfNRTgY",
    "outputId": "49dd6734-c015-43fe-d7d3-2c085e12d2fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias [-1.36869362]\n",
      "Weights [[ 1.56393259 -1.09655332]]\n",
      "Acc: 0.9347142857142857\n",
      "Acc: 0.9344285714285714\n",
      "Acc: 0.9348571428571428\n",
      "Acc: 0.9354285714285714\n",
      "Acc: 0.936\n",
      "Acc: 0.9364285714285714\n",
      "Acc: 0.9364285714285714\n",
      "Acc: 0.9364285714285714\n",
      "Acc: 0.9365714285714286\n",
      "Acc: 0.9365714285714286\n"
     ]
    }
   ],
   "source": [
    "model = MyTweetLogisticRegressionModel()\n",
    "model.fit(x=train_x,y=train_y,alpha=1e-4,record_epoch=True,epochs=10000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMydkTl2j/Mrpz020yTlHcY",
   "include_colab_link": true,
   "name": "Lab_7.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
